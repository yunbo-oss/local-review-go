# Local Review Go

用Go重写了黑马点评基本功能

我在ai的帮助下结合go的特性重构并优化了这个点评项目，并从单机架构升级为可水平扩展的分布式集群架构，。

以下是计划和正在进行的改动说明：

### 第一阶段：高并发与高性能

1. **启用 Redis 分布式布隆过滤器（解决缓存穿透）**
   - 废弃内存版 `BloomFilter` 实现，改为基于 Redis `BitMap` 模块的分布式布隆过滤器。
   - 在项目启动阶段预热加载所有店铺 ID 到布隆过滤器中。
   - 查询店铺详情前先校验布隆过滤器，若判定店铺 ID 不存在则直接返回错误，避免无效请求打到 Redis 与 MySQL。

2. **实现多级缓存 + 动态热点探测（缓解 Redis 热点 Key 瓶颈）**
   - 引入本地缓存库（如 `go-cache` 或 `BigCache`）作为 Redis 之上的 L1 缓存。
   - 在店铺等高频读取接口中增加“热点探测”逻辑（如滑动窗口计数、或未来接入 Sentinel 热点参数限流）。
   - 当发现某店铺访问量突增（Hot Key）时，自动将其写入本地缓存，设置短 TTL（如 5 秒），在突发流量时优先命中本地缓存，降低对 Redis 的压力。

---

### 第二阶段：稳定性与流量治理

3. **接入 Nginx + Sentinel 双重防护（高可用架构）**
   - 使用 Nginx 作为入口网关，配置轮询（Round-Robin）负载均衡，将流量分发到 3 个后端 Pod 实例。
   - 在应用内集成 Sentinel，对秒杀接口（如 `/voucher-order/seckill/:id`）做 QPS 限流（如 1000 QPS），对数据库访问层做熔断降级（快速失败），防止级联雪崩。

4. **完善定时任务 + 库存回滚（解决库存“死锁”和脏数据问题，融合苍穹外卖的思路）**
   - 引入 Cron 定时任务，每分钟扫描“超时未支付”的订单。
   - 在定时任务执行时使用 **分布式锁**，避免多个 Pod 同时执行任务导致重复回滚库存。
   - 业务流程：关闭超时订单 -> 恢复 MySQL 中的库存 -> 同步恢复 Redis 中的库存缓存，保证库存数据最终一致。

---

### 第三阶段：分布式集群适配

5. **改造 Redis Stream 消费者（适配多实例部署）**
   - 调整 Redis Stream 消费者组配置，将固定的消费者名称（例如原来的 `"c1"`）改为 `"c1-{UUID}"` 或 `"c1-{IP}"` 等带实例标识的名称。
   - 确保多 Pod 部署下，每个实例在同一消费者组中都有唯一的 Consumer Name，避免消息 ACK 与重试逻辑混乱。

6. **WebSocket 实时推送 + 集群广播（提升用户下单体验）**
   - 在前端下单后，通过 WebSocket 建立长连接，替代轮询接口查询订单/秒杀结果。
   - 在后端秒杀消费者处理完成后，通过 Redis Pub/Sub 进行广播，所有 Pod 接收消息后判断“该用户的 WebSocket 连接是否在本机”，如果在则立即通过 WebSocket 推送结果。
   - 通过 Pub/Sub + 本机连接路由的方式，实现多实例场景下的实时结果推送与集群内消息分发。